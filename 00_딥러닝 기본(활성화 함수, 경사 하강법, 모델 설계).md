## * 다층 퍼셉트론의 설계

<img src="/image/1.png" style="zoom:60%;" />

은닉층으로 퍼셉트론이 각각 자신의 가중치(w)와 바이어스(b)값을 보내고, 이 은닉층에서 모인 값들을 다시 시그모이드 함수를 이용해 최종값으로 낸다.



## * 오차 역전파(back propagation)

단일 퍼셉트론_(선형 회귀/로지스틱 회귀)일 경우 : 임의의 가중치를 선언하고 최소제곱법을 이용해 오차를 구한 뒤 이 오차가 최소인 지점으로 계속해서 조금씩 이동

<img src="/image/down.png" style="zoom:60%;" />

다층 퍼셉트론에서의 최적화 과정 : ```오차 역전파```



>오차 역전파란?

​    1) 임의의 초기 가중치(W1)를 준 뒤 y 값 계산

​    2) 계산 결과와 내가 원하는 값 사이의 오차를 구함

​    3) 경사 하강법을 이용해 바로 앞 가중치를 '오차가 작아지는 방향으로 업데이트'

​    4) 더 이상 오차가 줄어들지 않을 때까지 1)~3) 과정 반복

 

##### '오차가 작아지는 방향으로 업데이트'

= 미분 값이 0에 가까워 지는 방향

= 기울기가 0이 되는 방향

= 가중치에서 기울기를 뺐을 때 가중치의 변화가 전혀 없는 상태





##### 새 가중치 = 현 가중치 - '가중치에 대한 기울기'

==> ```오차 역전파``` : 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것

__________________________

====> 오차 역전파 방법으로 미분하면서 가중치를 수정하려면 기울기가 필요함

근데, 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제 발생





<img src="/image/down.png" style="zoom:60%;" />

## * 활성화 함수

=> 기울기 소실 문제를 해결하기 위해 활성화 함수를 시그모이드가 아닌 다른 함수로 대체

<img src="/image/2.png" style="zoom:60%;" />





## * 고급 경사 하강법

> 경사 하강법의 문제점 : 한 번 업데이트 할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많음(속도가 느려짐)

|     고급 경사 하강법     | 개요                                                         | 효과                    | 케라스 사용법                                                |
| :----------------------: | ------------------------------------------------------------ | ----------------------- | ------------------------------------------------------------ |
| SGD (확률적 경사 하강법) | 랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 업데이트를 하게 하는 것 | 속도 개선               | keras.optimizers.SGD(lr=0.1)                                 |
|     Momentum(모멘텀)     | 관성의 방향을 고려해 진동과 폭을 줄이는 효과                 | 정확도 개선             | keras.optimizers.SGD(lr=0.1,momentum=0.9)                    |
|  NAG(네스테로프 모멘텀)  | Momentum이 이동시킬 방향으로 미리 이동해서 gradient를 계산, 불필요한 이동을 줄이는 효과 | 정확도 개선             | keras.optimizers.SGD(lr=0.1,momentum=0.9, nesterov = True)   |
|   Adagrad(아다그라드)    | 변수의 업데이트가 잦으면 학습률을 적게하여 이동 보폭을 조절하는 방법 | 보폭 크기 개선          | keras.optimizers.Adagrad(lr=0.01,  epsilon = 1e-6            |
|  RMSProp(알엠에스프롭)   | Adagrad의 보폭 민감도를 보완한 방법                          | 보폭 크기 개선          | keras.optimizers.RMSProp(lr=0.001, rho=0.9,  epsilon = 1e-08, decay=0.0) |
|        Adam(아담)        | Momentum과 RMSProp 방법을 합친 방법                          | 정확도와 보폭 크기 개선 | keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,  epsilon = 1e-08, decay=0.0) |

 ===> 최근에 개발 된 ```Adam```이 가장 많이 사용되는 고급 경사 하강법이다.









# <모델 설계하기>



## * 입력층, 은닉층, 출력층

* `Dense(30, input_dim = 17, activation = 'relu')` : 입력층, 은닉층
  - 30 : 30개의 노드를 만든다.
  - input_dim = 17 : 입력 데이터로부터 몇 개의 값이 들어올지 정하는 것(keras는 입력층을 따로 만들지 않고 첫번 째 은닉층에 input_dim을 적어줌으로써 첫 번째 Dense가 은닉층+입력층의 역할을 한다.)
  - ==> 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다는 뜻
* `Dense(1,  activation = 'sigmoid') `: 출력층
  - 1 : 출력값



## * 모델 컴파일

- `model.compile(loss='mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])`

  : 환경 변수를 설정해줘서 모델을 실행하는 단계

  - loss : 

    - 평균 제곱 오차 계열__(수렴하기까지 속도가 많이 걸림) 

      |              함수              |         이름          |
      | :----------------------------: | :-------------------: |
      |       mean_squared_error       |    평균 제곱 오차     |
      |      mean_absolute_error       |    평균 절대 오차     |
      | mean_absolute_percentage_error | 평균 절대 백분율 오차 |
    | mean_squared_logarithmic_error |  평균 제곱 로그 오차  |
  
  - 교차 엔트로피 계열__(출력 값에 로그를 취해서, 오차가 커지면 수렴 속도가 빨라지고 오차가 작아지면 속도가 감소하는 것)
  
    |           함수           |                   이름                   |
    | :----------------------: | :--------------------------------------: |
  | categorical_crossentropy |   범주형 교차 엔트로피_(일반적인 분류)   |
    |   binary_crossentropy    | 이항 교차 엔트로피_(두 개의 클래스 예측) |

  - optimizer : 최적화 방법

  - metrics : 모델 수행 결과





## * 모델 실행

- `model.fit( X , Y, epochs = 30, batch_size = 10)`

  - epochs = 30 : 각 샘플이 처음부터 끝까지 30번 재사용될 때까지 실행을 반복하라는 것

    - 1 epoch : 학습 과정이 모든 샘플에 대해 한 번 실행 되는 것

  - batch_size = 10 : 전체 샘플을 10개씩 끊어서 집어 넣으라는 뜻

    - 너무 크면 학습 속도 느려짐
    - 너무 작으면 불안정해짐

    

## * k겹 교차 검증(k-fold cross validation)

(기존에는 train set : test set = 7 : 3으로 하면 test set을 30%만 써야 했다. )

> 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법   ====> 데이터의 100%를 test set으로 사용할 수 있다.



[5겹 교차 검증]   

<img src="/image/3.png" style="zoom:60%;" />

```python
from sklearn.model_selection import StratifiedKFold

n_fold = 10
skf = StratifiedKFold(n_splits = n_fold, shuffle = True, random_state = seed)
```

- StratifiedKFold : 데이터를 원하는 숫자만큼 쪼개 각각 학습셋과 테스트셋으로 사용되게 만드는 함수
- n_fold = 10 : 10개의 파일로 쪼개서 테스트한다.